{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvHXpAup_k9l"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"OpenAI tokenization lowerd.\""
      ],
      "metadata": {
        "id": "3_7bD3vZ_oSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = TreebankWordDetokenizer()\n",
        "t = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "lMr34bPIXvH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toks = t.tokenize(s)"
      ],
      "metadata": {
        "id": "FZZpk3bRXxXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZfjBydvX04S",
        "outputId": "55fcf7c1-249c-41b9-d86c-0ed806d26933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OpenAI', 'tokenization', 'lowerd', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d.detokenize(toks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gdSPp23BX2Bd",
        "outputId": "a0280735-205c-439d-eb7e-a1d81938d292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can't believe it's already 5 o'clock!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE"
      ],
      "metadata": {
        "id": "rhYGu1MtgIWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data: [https://github.com/brunoklein99/deep-learning-notes/blob/master/shakespeare.txt](https://github.com/brunoklein99/deep-learning-notes/blob/master/shakespeare.txt)"
      ],
      "metadata": {
        "id": "pz47sosug6ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing"
      ],
      "metadata": {
        "id": "E657wf0phM5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "FxXC2NGbhPG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "    vocab_size=1000,\n",
        "    min_frequency=1,\n",
        "    show_progress=True\n",
        ")"
      ],
      "metadata": {
        "id": "poxmUnxGhTSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "file_path = \"data.txt\"\n",
        "tokenizer.train(files=[file_path], trainer=trainer)"
      ],
      "metadata": {
        "id": "pP-HzkZ2hXBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",
        ")"
      ],
      "metadata": {
        "id": "fJQevwFlhZJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    \"department\",\n",
        "    \"OpenAI\",\n",
        "    \"tokenization\"]"
      ],
      "metadata": {
        "id": "PXjTcAAvhe6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in test_cases:\n",
        "    encoded = tokenizer.encode(text)\n",
        "    print(f\"'{text}' → {encoded.tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJKIS4u_gKHC",
        "outputId": "76e97d68-348f-4e54-9181-bab5b66ac7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'department' → ['[CLS]', 'de', 'part', 'ment', '[SEP]']\n",
            "'OpenAI' → ['[CLS]', 'O', 'pen', 'A', 'I', '[SEP]']\n",
            "'tokenization' → ['[CLS]', 'to', 'ken', 'i', 'z', 'ation', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word piece"
      ],
      "metadata": {
        "id": "lw04iatZsFYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer"
      ],
      "metadata": {
        "id": "W-fOrtrTsJUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    min_frequency=2\n",
        ")"
      ],
      "metadata": {
        "id": "fsZprU1ZsMSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\"data.txt\"]\n",
        "tokenizer.train(files, trainer)"
      ],
      "metadata": {
        "id": "MtfqEzgmsSmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B [SEP]\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "6SpsdIzKsVIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    \"department\",\n",
        "    \"OpenAI\",\n",
        "    \"through\"]"
      ],
      "metadata": {
        "id": "RIh_AOR_sZoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in test_cases:\n",
        "    encoded = tokenizer.encode(text)\n",
        "    print(f\"'{text}' → {encoded.tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdVXQDcvsEmp",
        "outputId": "bbca6347-e879-45a0-fbad-b3d94540fdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'department' → ['[CLS]', 'de', '##p', '##art', '##ment', '[SEP]']\n",
            "'OpenAI' → ['[CLS]', '[UNK]', '[SEP]']\n",
            "'through' → ['[CLS]', 'th', '##rough', '[SEP]']\n"
          ]
        }
      ]
    }
  ]
}